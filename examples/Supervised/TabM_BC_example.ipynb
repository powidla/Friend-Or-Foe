{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L3W3oDJh93z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rtdl_num_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "mmuSr8ziicI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from typing import Any, Literal\n",
        "\n",
        "import rtdl_num_embeddings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "fwSqzPuLiCYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_rsqrt_uniform_(x: Tensor, d: int) -> Tensor:\n",
        "    assert d > 0\n",
        "    d_rsqrt = d**-0.5\n",
        "    return nn.init.uniform_(x, -d_rsqrt, d_rsqrt)\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def init_random_signs_(x: Tensor) -> Tensor:\n",
        "    return x.bernoulli_(0.5).mul_(2).add_(-1)\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# Modules\n",
        "# ======================================================================================\n",
        "class NLinear(nn.Module):\n",
        "    \"\"\"N linear layers applied in parallel to N disjoint parts of the input.\n",
        "\n",
        "    **Shape**\n",
        "\n",
        "    - Input: ``(B, N, in_features)``\n",
        "    - Output: ``(B, N, out_features)``\n",
        "\n",
        "    The i-th linear layer is applied to the i-th matrix of the shape (B, in_features).\n",
        "\n",
        "    Technically, this is a simplified version of delu.nn.NLinear:\n",
        "    https://yura52.github.io/delu/stable/api/generated/delu.nn.NLinear.html.\n",
        "    The difference is that this layer supports only 3D inputs\n",
        "    with exactly one batch dimension. By contrast, delu.nn.NLinear supports\n",
        "    any number of batch dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, n: int, in_features: int, out_features: int, bias: bool = True\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(n, in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.empty(n, out_features)) if bias else None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        d = self.weight.shape[-2]\n",
        "        init_rsqrt_uniform_(self.weight, d)\n",
        "        if self.bias is not None:\n",
        "            init_rsqrt_uniform_(self.bias, d)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.ndim == 3\n",
        "        assert x.shape[-(self.weight.ndim - 1) :] == self.weight.shape[:-1]\n",
        "\n",
        "        x = x.transpose(0, 1)\n",
        "        x = x @ self.weight\n",
        "        x = x.transpose(0, 1)\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias\n",
        "        return x\n",
        "\n",
        "\n",
        "class OneHotEncoding0d(nn.Module):\n",
        "    # Input:  (*, n_cat_features=len(cardinalities))\n",
        "    # Output: (*, sum(cardinalities))\n",
        "\n",
        "    def __init__(self, cardinalities: list[int]) -> None:\n",
        "        super().__init__()\n",
        "        self._cardinalities = cardinalities\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        assert x.ndim >= 1\n",
        "        assert x.shape[-1] == len(self._cardinalities)\n",
        "\n",
        "        return torch.cat(\n",
        "            [\n",
        "                # NOTE\n",
        "                # This is a quick hack to support out-of-vocabulary categories.\n",
        "                #\n",
        "                # Recall that lib.data.transform_cat encodes categorical features\n",
        "                # as follows:\n",
        "                # - In-vocabulary values receive indices from `range(cardinality)`.\n",
        "                # - All out-of-vocabulary values (i.e. new categories in validation\n",
        "                #   and test data that are not presented in the training data)\n",
        "                #   receive the index `cardinality`.\n",
        "                #\n",
        "                # As such, the line below will produce the standard one-hot encoding for\n",
        "                # known categories, and the all-zeros encoding for unknown categories.\n",
        "                # This may not be the best approach to deal with unknown values,\n",
        "                # but should be enough for our purposes.\n",
        "                nn.functional.one_hot(x[..., i], cardinality + 1)[..., :-1]\n",
        "                for i, cardinality in enumerate(self._cardinalities)\n",
        "            ],\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "\n",
        "class ScaleEnsemble(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        k: int,\n",
        "        d: int,\n",
        "        *,\n",
        "        init: Literal['ones', 'normal', 'random-signs'],\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(k, d))\n",
        "        self._weight_init = init\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        if self._weight_init == 'ones':\n",
        "            nn.init.ones_(self.weight)\n",
        "        elif self._weight_init == 'normal':\n",
        "            nn.init.normal_(self.weight)\n",
        "        elif self._weight_init == 'random-signs':\n",
        "            init_random_signs_(self.weight)\n",
        "        else:\n",
        "            raise ValueError(f'Unknown weight_init: {self._weight_init}')\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        assert x.ndim >= 2\n",
        "        return x * self.weight\n",
        "\n",
        "\n",
        "class LinearEfficientEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    This layer is a more configurable version of the \"BatchEnsemble\" layer\n",
        "    from the paper\n",
        "    \"BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning\"\n",
        "    (link: https://arxiv.org/abs/2002.06715).\n",
        "\n",
        "    First, this layer allows to select only some of the \"ensembled\" parts:\n",
        "    - the input scaling  (r_i in the BatchEnsemble paper)\n",
        "    - the output scaling (s_i in the BatchEnsemble paper)\n",
        "    - the output bias    (not mentioned in the BatchEnsemble paper,\n",
        "                          but is presented in public implementations)\n",
        "\n",
        "    Second, the initialization of the scaling weights is configurable\n",
        "    through the `scaling_init` argument.\n",
        "\n",
        "    NOTE\n",
        "    The term \"adapter\" is used in the TabM paper only to tell the story.\n",
        "    The original BatchEnsemble paper does NOT use this term. So this class also\n",
        "    avoids the term \"adapter\".\n",
        "    \"\"\"\n",
        "\n",
        "    r: None | Tensor\n",
        "    s: None | Tensor\n",
        "    bias: None | Tensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        bias: bool = True,\n",
        "        *,\n",
        "        k: int,\n",
        "        ensemble_scaling_in: bool,\n",
        "        ensemble_scaling_out: bool,\n",
        "        ensemble_bias: bool,\n",
        "        scaling_init: Literal['ones', 'random-signs'],\n",
        "    ):\n",
        "        assert k > 0\n",
        "        if ensemble_bias:\n",
        "            assert bias\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.register_parameter(\n",
        "            'r',\n",
        "            (\n",
        "                nn.Parameter(torch.empty(k, in_features))\n",
        "                if ensemble_scaling_in\n",
        "                else None\n",
        "            ),  # type: ignore[code]\n",
        "        )\n",
        "        self.register_parameter(\n",
        "            's',\n",
        "            (\n",
        "                nn.Parameter(torch.empty(k, out_features))\n",
        "                if ensemble_scaling_out\n",
        "                else None\n",
        "            ),  # type: ignore[code]\n",
        "        )\n",
        "        self.register_parameter(\n",
        "            'bias',\n",
        "            (\n",
        "                nn.Parameter(torch.empty(out_features))  # type: ignore[code]\n",
        "                if bias and not ensemble_bias\n",
        "                else nn.Parameter(torch.empty(k, out_features))\n",
        "                if ensemble_bias\n",
        "                else None\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.k = k\n",
        "        self.scaling_init = scaling_init\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init_rsqrt_uniform_(self.weight, self.in_features)\n",
        "        scaling_init_fn = {'ones': nn.init.ones_, 'random-signs': init_random_signs_}[\n",
        "            self.scaling_init\n",
        "        ]\n",
        "        if self.r is not None:\n",
        "            scaling_init_fn(self.r)\n",
        "        if self.s is not None:\n",
        "            scaling_init_fn(self.s)\n",
        "        if self.bias is not None:\n",
        "            bias_init = torch.empty(\n",
        "                # NOTE: the shape of bias_init is (out_features,) not (k, out_features).\n",
        "                # It means that all biases have the same initialization.\n",
        "                # This is similar to having one shared bias plus\n",
        "                # k zero-initialized non-shared biases.\n",
        "                self.out_features,\n",
        "                dtype=self.weight.dtype,\n",
        "                device=self.weight.device,\n",
        "            )\n",
        "            bias_init = init_rsqrt_uniform_(bias_init, self.in_features)\n",
        "            with torch.inference_mode():\n",
        "                self.bias.copy_(bias_init)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x.shape == (B, K, D)\n",
        "        assert x.ndim == 3\n",
        "\n",
        "        # >>> The equation (5) from the BatchEnsemble paper (arXiv v2).\n",
        "        if self.r is not None:\n",
        "            x = x * self.r\n",
        "        x = x @ self.weight.T\n",
        "        if self.s is not None:\n",
        "            x = x * self.s\n",
        "        # <<<\n",
        "\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VOhL9PPHiNjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        d_in: None | int = None,\n",
        "        d_out: None | int = None,\n",
        "        n_blocks: int,\n",
        "        d_block: int,\n",
        "        dropout: float,\n",
        "        activation: str = 'ReLU',\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        d_first = d_block if d_in is None else d_in\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(d_first if i == 0 else d_block, d_block),\n",
        "                    getattr(nn, activation)(),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "                for i in range(n_blocks)\n",
        "            ]\n",
        "        )\n",
        "        self.output = None if d_out is None else nn.Linear(d_block, d_out)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        if self.output is not None:\n",
        "            x = self.output(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def make_efficient_ensemble(module: nn.Module, **kwargs) -> None:\n",
        "    \"\"\"Replace torch.nn.Linear modules with LinearEfficientEnsemble.\n",
        "\n",
        "    NOTE\n",
        "    In the paper, there are no experiments with networks with normalization layers.\n",
        "    Perhaps, their trainable weights (the affine transformations) also need\n",
        "    \"ensemblification\" as in the paper about \"FiLM-Ensemble\".\n",
        "    Additional experiments are required to make conclusions.\n",
        "    \"\"\"\n",
        "    for name, submodule in list(module.named_children()):\n",
        "        if isinstance(submodule, nn.Linear):\n",
        "            module.add_module(\n",
        "                name,\n",
        "                LinearEfficientEnsemble(\n",
        "                    in_features=submodule.in_features,\n",
        "                    out_features=submodule.out_features,\n",
        "                    bias=submodule.bias is not None,\n",
        "                    **kwargs,\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            make_efficient_ensemble(submodule, **kwargs)\n",
        "\n",
        "\n",
        "def _get_first_ensemble_layer(backbone: MLP) -> LinearEfficientEnsemble:\n",
        "    if isinstance(backbone, MLP):\n",
        "        return backbone.blocks[0][0]  # type: ignore[code]\n",
        "    else:\n",
        "        raise RuntimeError(f'Unsupported backbone: {backbone}')\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def _init_first_adapter(\n",
        "    weight: Tensor,\n",
        "    distribution: Literal['normal', 'random-signs'],\n",
        "    init_sections: list[int],\n",
        ") -> None:\n",
        "    \"\"\"Initialize the first adapter.\n",
        "\n",
        "    NOTE\n",
        "    The `init_sections` argument is a historical artifact that accidentally leaked\n",
        "    from irrelevant experiments to the final models. Perhaps, the code related\n",
        "    to `init_sections` can be simply removed, but this was not tested.\n",
        "    \"\"\"\n",
        "    assert weight.ndim == 2\n",
        "    assert weight.shape[1] == sum(init_sections)\n",
        "\n",
        "    if distribution == 'normal':\n",
        "        init_fn_ = nn.init.normal_\n",
        "    elif distribution == 'random-signs':\n",
        "        init_fn_ = init_random_signs_\n",
        "    else:\n",
        "        raise ValueError(f'Unknown distribution: {distribution}')\n",
        "\n",
        "    section_bounds = [0, *torch.tensor(init_sections).cumsum(0).tolist()]\n",
        "    for i in range(len(init_sections)):\n",
        "        # NOTE\n",
        "        # As noted above, this section-based initialization is an arbitrary historical\n",
        "        # artifact. Consider the first adapter of one ensemble member.\n",
        "        # This adapter vector is implicitly split into \"sections\",\n",
        "        # where one section corresponds to one feature. The code below ensures that\n",
        "        # the adapter weights in one section are initialized with the same random value\n",
        "        # from the given distribution.\n",
        "        w = torch.empty((len(weight), 1), dtype=weight.dtype, device=weight.device)\n",
        "        init_fn_(w)\n",
        "        weight[:, section_bounds[i] : section_bounds[i + 1]] = w\n",
        "\n",
        "\n",
        "_CUSTOM_MODULES = {\n",
        "    # https://docs.python.org/3/library/stdtypes.html#definition.__name__\n",
        "    CustomModule.__name__: CustomModule\n",
        "    for CustomModule in [\n",
        "        rtdl_num_embeddings.LinearEmbeddings,\n",
        "        rtdl_num_embeddings.LinearReLUEmbeddings,\n",
        "        rtdl_num_embeddings.PeriodicEmbeddings,\n",
        "        rtdl_num_embeddings.PiecewiseLinearEmbeddings,\n",
        "        MLP,\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def make_module(type: str, *args, **kwargs) -> nn.Module:\n",
        "    Module = getattr(nn, type, None)\n",
        "    if Module is None:\n",
        "        Module = _CUSTOM_MODULES[type]\n",
        "    return Module(*args, **kwargs)\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# Optimization\n",
        "# ======================================================================================\n",
        "def default_zero_weight_decay_condition(\n",
        "    module_name: str, module: nn.Module, parameter_name: str, parameter: nn.Parameter\n",
        "):\n",
        "    from rtdl_num_embeddings import _Periodic\n",
        "\n",
        "    del module_name, parameter\n",
        "    return parameter_name.endswith('bias') or isinstance(\n",
        "        module,\n",
        "        nn.BatchNorm1d\n",
        "        | nn.LayerNorm\n",
        "        | nn.InstanceNorm1d\n",
        "        | rtdl_num_embeddings.LinearEmbeddings\n",
        "        | rtdl_num_embeddings.LinearReLUEmbeddings\n",
        "        | _Periodic,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_parameter_groups(\n",
        "    module: nn.Module,\n",
        "    zero_weight_decay_condition=default_zero_weight_decay_condition,\n",
        "    custom_groups: None | list[dict[str, Any]] = None,\n",
        ") -> list[dict[str, Any]]:\n",
        "    if custom_groups is None:\n",
        "        custom_groups = []\n",
        "    custom_params = frozenset(\n",
        "        itertools.chain.from_iterable(group['params'] for group in custom_groups)\n",
        "    )\n",
        "    assert len(custom_params) == sum(\n",
        "        len(group['params']) for group in custom_groups\n",
        "    ), 'Parameters in custom_groups must not intersect'\n",
        "    zero_wd_params = frozenset(\n",
        "        p\n",
        "        for mn, m in module.named_modules()\n",
        "        for pn, p in m.named_parameters()\n",
        "        if p not in custom_params and zero_weight_decay_condition(mn, m, pn, p)\n",
        "    )\n",
        "    default_group = {\n",
        "        'params': [\n",
        "            p\n",
        "            for p in module.parameters()\n",
        "            if p not in custom_params and p not in zero_wd_params\n",
        "        ]\n",
        "    }\n",
        "    return [\n",
        "        default_group,\n",
        "        {'params': list(zero_wd_params), 'weight_decay': 0.0},\n",
        "        *custom_groups,\n",
        "    ]"
      ],
      "metadata": {
        "id": "RghPXfSNiNl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#core"
      ],
      "metadata": {
        "id": "aoqa-XAuifN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"MLP & TabM.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        n_num_features: int,\n",
        "        cat_cardinalities: list[int],\n",
        "        n_classes: None | int,\n",
        "        backbone: dict,\n",
        "        bins: None | list[Tensor],  # For piecewise-linear encoding/embeddings.\n",
        "        num_embeddings: None | dict = None,\n",
        "        arch_type: Literal[\n",
        "            # Plain feed-forward network without any kind of ensembling.\n",
        "            'plain',\n",
        "            #\n",
        "            # TabM-mini\n",
        "            'tabm-mini',\n",
        "            #\n",
        "            # TabM-mini. The first adapter is initialized from the normal distribution.\n",
        "            # This is used in Section 5.1 of the paper.\n",
        "            'tabm-mini-normal',\n",
        "            #\n",
        "            # TabM\n",
        "            'tabm',\n",
        "            #\n",
        "            # TabM. The first adapter is initialized from the normal distribution.\n",
        "            # This variation is not used in the paper, but there is a preliminary\n",
        "            # evidence that may be a better default strategy.\n",
        "            'tabm-normal',\n",
        "        ],\n",
        "        k: None | int = None,\n",
        "    ) -> None:\n",
        "        # >>> Validate arguments.\n",
        "        assert n_num_features >= 0\n",
        "        assert n_num_features or cat_cardinalities\n",
        "        if arch_type == 'plain':\n",
        "            assert k is None\n",
        "        else:\n",
        "            assert k is not None\n",
        "            assert k > 0\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # >>> Continuous (numerical) features\n",
        "        first_adapter_sections = []  # See the comment in `_init_first_adapter`.\n",
        "\n",
        "        if n_num_features == 0:\n",
        "            assert bins is None\n",
        "            self.num_module = None\n",
        "            d_num = 0\n",
        "\n",
        "        elif num_embeddings is None:\n",
        "            assert bins is None\n",
        "            self.num_module = None\n",
        "            d_num = n_num_features\n",
        "            first_adapter_sections.extend(1 for _ in range(n_num_features))\n",
        "\n",
        "        else:\n",
        "            if bins is None:\n",
        "                self.num_module = make_module(\n",
        "                    **num_embeddings, n_features=n_num_features\n",
        "                )\n",
        "            else:\n",
        "                assert num_embeddings['type'].startswith('PiecewiseLinearEmbeddings')\n",
        "                self.num_module = make_module(**num_embeddings, bins=bins)\n",
        "            d_num = n_num_features * num_embeddings['d_embedding']\n",
        "            first_adapter_sections.extend(\n",
        "                num_embeddings['d_embedding'] for _ in range(n_num_features)\n",
        "            )\n",
        "\n",
        "        # >>> Categorical features\n",
        "        self.cat_module = (\n",
        "            OneHotEncoding0d(cat_cardinalities) if cat_cardinalities else None\n",
        "        )\n",
        "        first_adapter_sections.extend(cat_cardinalities)\n",
        "        d_cat = sum(cat_cardinalities)\n",
        "\n",
        "        # >>> Backbone\n",
        "        d_flat = d_num + d_cat\n",
        "        self.minimal_ensemble_adapter = None\n",
        "        # Any backbone can be here but we provide only MLP\n",
        "        self.backbone = make_module(d_in=d_flat, **backbone)\n",
        "\n",
        "        if arch_type != 'plain':\n",
        "            assert k is not None\n",
        "            first_adapter_init = (\n",
        "                'normal'\n",
        "                if arch_type in ('tabm-mini-normal', 'tabm-normal')\n",
        "                # For other arch_types, the initialization depends\n",
        "                # on the presense of num_embeddings.\n",
        "                else 'random-signs'\n",
        "                if num_embeddings is None\n",
        "                else 'normal'\n",
        "            )\n",
        "\n",
        "            if arch_type in ('tabm-mini', 'tabm-mini-normal'):\n",
        "                # Minimal ensemble\n",
        "                self.minimal_ensemble_adapter = ScaleEnsemble(\n",
        "                    k,\n",
        "                    d_flat,\n",
        "                    init='random-signs' if num_embeddings is None else 'normal',\n",
        "                )\n",
        "                _init_first_adapter(\n",
        "                    self.minimal_ensemble_adapter.weight,  # type: ignore[code]\n",
        "                    first_adapter_init,\n",
        "                    first_adapter_sections,\n",
        "                )\n",
        "\n",
        "            elif arch_type in ('tabm', 'tabm-normal'):\n",
        "                # Like BatchEnsemble, but all multiplicative adapters,\n",
        "                # except for the very first one, are initialized with ones.\n",
        "                make_efficient_ensemble(\n",
        "                    self.backbone,\n",
        "                    k=k,\n",
        "                    ensemble_scaling_in=True,\n",
        "                    ensemble_scaling_out=True,\n",
        "                    ensemble_bias=True,\n",
        "                    scaling_init='ones',\n",
        "                )\n",
        "                _init_first_adapter(\n",
        "                    _get_first_ensemble_layer(self.backbone).r,  # type: ignore[code]\n",
        "                    first_adapter_init,\n",
        "                    first_adapter_sections,\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f'Unknown arch_type: {arch_type}')\n",
        "\n",
        "        # >>> Output\n",
        "        d_block = backbone['d_block']\n",
        "        d_out = 1 if n_classes is None else n_classes\n",
        "        self.output = (\n",
        "            nn.Linear(d_block, d_out)\n",
        "            if arch_type == 'plain'\n",
        "            else NLinear(k, d_block, d_out)  # type: ignore[code]\n",
        "        )\n",
        "\n",
        "        # >>>\n",
        "        self.arch_type = arch_type\n",
        "        self.k = k\n",
        "\n",
        "    def forward(\n",
        "        self, x_num: None | Tensor = None, x_cat: None | Tensor = None\n",
        "    ) -> Tensor:\n",
        "        x = []\n",
        "        if x_num is not None:\n",
        "            x.append(x_num if self.num_module is None else self.num_module(x_num))\n",
        "        if x_cat is None:\n",
        "            assert self.cat_module is None\n",
        "        else:\n",
        "            assert self.cat_module is not None\n",
        "            x.append(self.cat_module(x_cat).float())\n",
        "        x = torch.column_stack([x_.flatten(1, -1) for x_ in x])\n",
        "\n",
        "        if self.k is not None:\n",
        "            x = x[:, None].expand(-1, self.k, -1)  # (B, D) -> (B, K, D)\n",
        "            if self.minimal_ensemble_adapter is not None:\n",
        "                x = self.minimal_ensemble_adapter(x)\n",
        "        else:\n",
        "            assert self.minimal_ensemble_adapter is None\n",
        "\n",
        "        x = self.backbone(x)\n",
        "        x = self.output(x)\n",
        "        if self.k is None:\n",
        "            # Adjust the output shape for plain networks to make them compatible\n",
        "            # with the rest of the script (loss, metrics, predictions, ...).\n",
        "            # (B, D_OUT) -> (B, 1, D_OUT)\n",
        "            x = x[:, None]\n",
        "        return x"
      ],
      "metadata": {
        "id": "i_EENtn5iY-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "uAHb_8F-kSwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from typing import Literal, NamedTuple\n",
        "\n",
        "import numpy as np\n",
        "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
        "import scipy.special\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "from torch import Tensor\n",
        "from tqdm.std import tqdm\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "PNeKR65kkSVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.resetwarnings()\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed + 1)\n",
        "torch.manual_seed(seed + 2)\n",
        "pass"
      ],
      "metadata": {
        "id": "Dhrc3sxXkVye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# >>> Dataset.\n",
        "TaskType = Literal['regression', 'binclass', 'multiclass']\n",
        "\n",
        "# Regression.\n",
        "task_type: TaskType = 'regression'\n",
        "n_classes = None\n",
        "dataset = sklearn.datasets.fetch_california_housing()\n",
        "X_cont: np.ndarray = dataset['data']\n",
        "Y: np.ndarray = dataset['target']\n",
        "\n",
        "# Classification.\n",
        "n_classes = 4\n",
        "assert n_classes >= 2\n",
        "task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
        "X_cont, Y = sklearn.datasets.make_classification(\n",
        "    n_samples=20000,\n",
        "    n_features=8,\n",
        "    n_classes=n_classes,\n",
        "    n_informative=3,\n",
        "    n_redundant=2,\n",
        ")\n",
        "\n",
        "task_is_regression = task_type == 'multiclass'\n",
        "\n",
        "# >>> Continuous features.\n",
        "X_cont: np.ndarray = X_cont.astype(np.float32)\n",
        "n_cont_features = X_cont.shape[1]\n",
        "\n",
        "# >>> Categorical features.\n",
        "# NOTE: the above datasets do not have categorical features, however,\n",
        "# for the demonstration purposes, it is possible to generate them.\n",
        "cat_cardinalities = [\n",
        "    # NOTE: uncomment the two lines below to add two categorical features.\n",
        "    # 4,  # Allowed values: [0, 1, 2, 3].\n",
        "    # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
        "]\n",
        "X_cat = (\n",
        "    np.column_stack(\n",
        "        [np.random.randint(0, c, (len(X_cont),)) for c in cat_cardinalities]\n",
        "    )\n",
        "    if cat_cardinalities\n",
        "    else None\n",
        ")\n",
        "\n",
        "# >>> Labels.\n",
        "if task_type == 'regression':\n",
        "    Y = Y.astype(np.float32)\n",
        "else:\n",
        "    assert n_classes is not None\n",
        "    Y = Y.astype(np.int64)\n",
        "    assert set(Y.tolist()) == set(\n",
        "        range(n_classes)\n",
        "    ), 'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
        "\n",
        "# >>> Split the dataset.\n",
        "all_idx = np.arange(len(Y))\n",
        "trainval_idx, test_idx = sklearn.model_selection.train_test_split(\n",
        "    all_idx, train_size=0.8\n",
        ")\n",
        "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
        "    trainval_idx, train_size=0.8\n",
        ")\n",
        "data_numpy = {\n",
        "    'train': {'x_cont': X_cont[train_idx], 'y': Y[train_idx]},\n",
        "    'val': {'x_cont': X_cont[val_idx], 'y': Y[val_idx]},\n",
        "    'test': {'x_cont': X_cont[test_idx], 'y': Y[test_idx]},\n",
        "}\n",
        "if X_cat is not None:\n",
        "    data_numpy['train']['x_cat'] = X_cat[train_idx]\n",
        "    data_numpy['val']['x_cat'] = X_cat[val_idx]\n",
        "    data_numpy['test']['x_cat'] = X_cat[test_idx]"
      ],
      "metadata": {
        "id": "TsTTWw6vkYdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb29ea33-41dc-46c8-820f-a246a14d2133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature preprocessing.\n",
        "# NOTE\n",
        "# The choice between preprocessing strategies depends on a task and a model.\n",
        "\n",
        "# Simple preprocessing strategy.\n",
        "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
        "#     data_numpy['train']['x_cont']\n",
        "# )\n",
        "\n",
        "# Advanced preprocessing strategy.\n",
        "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
        "X_cont_train_numpy = data_numpy['train']['x_cont']\n",
        "noise = (\n",
        "    np.random.default_rng(0)\n",
        "    .normal(0.0, 1e-5, X_cont_train_numpy.shape)\n",
        "    .astype(X_cont_train_numpy.dtype)\n",
        ")\n",
        "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
        "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
        "    output_distribution='normal',\n",
        "    subsample=10**9,\n",
        ").fit(X_cont_train_numpy + noise)\n",
        "del X_cont_train_numpy\n",
        "\n",
        "# Apply the preprocessing.\n",
        "for part in data_numpy:\n",
        "    data_numpy[part]['x_cont'] = preprocessing.transform(data_numpy[part]['x_cont'])\n",
        "\n",
        "\n",
        "# Label preprocessing.\n",
        "class RegressionLabelStats(NamedTuple):\n",
        "    mean: float\n",
        "    std: float\n",
        "\n",
        "\n",
        "Y_train = data_numpy['train']['y'].copy()\n",
        "if task_type == 'regression':\n",
        "    # For regression tasks, it is highly recommended to standardize the training labels.\n",
        "    regression_label_stats = RegressionLabelStats(\n",
        "        Y_train.mean().item(), Y_train.std().item()\n",
        "    )\n",
        "    Y_train = (Y_train - regression_label_stats.mean) / regression_label_stats.std\n",
        "else:\n",
        "    regression_label_stats = None"
      ],
      "metadata": {
        "id": "z-HN9UIekepN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLdnaNw-80y",
        "outputId": "8ec34ba5-f203-4acd-af7d-61755461b649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "mgGH0s8A4vCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *pth*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgfIxSpgV438",
        "outputId": "b7380616-894b-475e-a150-7b1b103fae62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*pth*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv(\"/content/drive/MyDrive/TL-I/AGORA/100/X_train_TL-I.csv\")\n",
        "X_val = pd.read_csv(\"/content/drive/MyDrive/TL-I/AGORA/100/X_val_TL-I.csv\")\n",
        "X_test = pd.read_csv(\"/content/drive/MyDrive/TL-I/AGORA/100/X_test_TL-I.csv\")\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/TL-I/AGORA/100/y_train_TL-I.csv\")\n",
        "y_val = pd.read_csv(\"/content/drive/MyDrive/TL-I/AGORA/100/y_val_TL-I.csv\")\n",
        "y_test = pd.read_csv(\"/content/drive/MyDrive/TL-I/AGORA/100/y_test_TL-I.csv\")"
      ],
      "metadata": {
        "id": "QAPMOg3n_U5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1RSxgX5IQ6hy",
        "outputId": "4a241867-adc6-4dee-9998-21433b678e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        label\n",
              "0           1\n",
              "1           1\n",
              "2           0\n",
              "3           1\n",
              "4           1\n",
              "...       ...\n",
              "131849      1\n",
              "131850      0\n",
              "131851      0\n",
              "131852      0\n",
              "131853      1\n",
              "\n",
              "[131854 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8032791e-902f-4d57-b284-6ad0c2481681\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131849</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131850</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131851</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131852</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131853</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>131854 rows Ã— 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8032791e-902f-4d57-b284-6ad0c2481681')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8032791e-902f-4d57-b284-6ad0c2481681 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8032791e-902f-4d57-b284-6ad0c2481681');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0b751efe-d957-4ae7-9008-c107eeb4d1b1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0b751efe-d957-4ae7-9008-c107eeb4d1b1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0b751efe-d957-4ae7-9008-c107eeb4d1b1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d32c1f10-33e6-4d85-ba10-4ce404b571b6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('y_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d32c1f10-33e6-4d85-ba10-4ce404b571b6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('y_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "y_test"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.replace(2, 0)\n",
        "y_val = y_val.replace(2, 0)\n",
        "y_test = y_test.replace(2, 0)"
      ],
      "metadata": {
        "id": "UDJxCx4HP5ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.replace(2, 1)\n",
        "y_val = y_val.replace(2, 1)\n",
        "y_test = y_test.replace(2, 1)"
      ],
      "metadata": {
        "id": "e1nt5Vv8LeE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_numpy = {\n",
        "    'train': {'x_cont': X_train.values, 'y': y_train.values},\n",
        "    'val': {'x_cont': X_val.values, 'y': y_val.values},\n",
        "    'test': {'x_cont': X_test.values, 'y': y_test.values},\n",
        "}"
      ],
      "metadata": {
        "id": "WFu0QaAC_qdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert data to tensors\n",
        "data = {\n",
        "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
        "    for part in data_numpy\n",
        "}\n",
        "Y_train = torch.as_tensor(y_train.values, device=device)\n",
        "if task_type == 'regression':\n",
        "    for part in data:\n",
        "        data[part]['y'] = data[part]['y'].float()\n",
        "    Y_train = Y_train.float()\n",
        "\n",
        "# Automatic mixed precision (AMP)\n",
        "# torch.float16 is implemented for completeness,\n",
        "# but it was not tested in the project,\n",
        "# so torch.bfloat16 is used by default.\n",
        "amp_dtype = (\n",
        "    torch.bfloat16\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else torch.float16\n",
        "    if torch.cuda.is_available()\n",
        "    else None\n",
        ")\n",
        "# Changing False to True will result in faster training on compatible hardware.\n",
        "amp_enabled = False and amp_dtype is not None\n",
        "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
        "\n",
        "# torch.compile\n",
        "compile_model = False\n",
        "\n",
        "# fmt: off\n",
        "print(\n",
        "    f'Device:        {device.type.upper()}'\n",
        "    f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
        "    f'\\ntorch.compile: {compile_model}'\n",
        ")\n",
        "# fmt: on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaf6AiJ__o4Y",
        "outputId": "49242c7f-d711-46b9-9212-eec8661afbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:        CUDA\n",
            "AMP:           False (dtype: torch.bfloat16)\n",
            "torch.compile: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose one of the two configurations below.\n",
        "\n",
        "# TabM\n",
        "arch_type = 'tabm'\n",
        "bins = None\n",
        "\n",
        "# TabM-mini with the piecewise-linear embeddings.\n",
        "# arch_type = 'tabm-mini'\n",
        "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
        "\n",
        "model = Model(\n",
        "    n_num_features=X_train.shape[1],\n",
        "    cat_cardinalities=cat_cardinalities,\n",
        "    n_classes=n_classes,\n",
        "    backbone={\n",
        "        'type': 'MLP',\n",
        "        'n_blocks': 3 if bins is None else 2,\n",
        "        'd_block': 512,\n",
        "        'dropout': 0.1,\n",
        "    },\n",
        "    bins=bins,\n",
        "    num_embeddings=(\n",
        "        None\n",
        "        if bins is None\n",
        "        else {\n",
        "            'type': 'PiecewiseLinearEmbeddings',\n",
        "            'd_embedding': 16,\n",
        "            'activation': False,\n",
        "            'version': 'B',\n",
        "        }\n",
        "    ),\n",
        "    arch_type=arch_type,\n",
        "    k=32,\n",
        ").to(device)\n",
        "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
        "\n",
        "if compile_model:\n",
        "    # NOTE\n",
        "    # `torch.compile` is intentionally called without the `mode` argument\n",
        "    # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
        "    model = torch.compile(model)\n",
        "    evaluation_mode = torch.no_grad\n",
        "else:\n",
        "    evaluation_mode = torch.inference_mode"
      ],
      "metadata": {
        "id": "ExdUqKQ0kjG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
        "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
        "    return (\n",
        "        model(\n",
        "            data[part]['x_cont'][idx],\n",
        "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
        "        )\n",
        "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
        "        .float()\n",
        "    )\n",
        "\n",
        "\n",
        "base_loss_fn = F.mse_loss if task_type == 'regression' else F.cross_entropy\n",
        "\n",
        "\n",
        "def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
        "    # TabM produces k predictions per object. Each of them must be trained separately.\n",
        "    # (regression)     y_pred.shape == (batch_size, k)\n",
        "    # (classification) y_pred.shape == (batch_size, k, n_classes)\n",
        "    k = y_pred.shape[-1 if task_type == 'regression' else -2]\n",
        "    return base_loss_fn(y_pred.flatten(0, 1), y_true.repeat_interleave(k))\n",
        "\n",
        "\n",
        "@evaluation_mode()\n",
        "def evaluate(part: str) -> float:\n",
        "    model.eval()\n",
        "\n",
        "    # When using torch.compile, you may need to reduce the evaluation batch size.\n",
        "    eval_batch_size = 80\n",
        "    y_pred: np.ndarray = (\n",
        "        torch.cat(\n",
        "            [\n",
        "                apply_model(part, idx)\n",
        "                for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
        "                    eval_batch_size\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "    )\n",
        "    if task_type == 'regression':\n",
        "        # Transform the predictions back to the original label space.\n",
        "        assert regression_label_stats is not None\n",
        "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
        "\n",
        "    # Compute the mean of the k predictions.\n",
        "    if task_type != 'regression':\n",
        "        # For classification, the mean must be computed in the probabily space.\n",
        "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
        "    y_pred = y_pred.mean(1)\n",
        "\n",
        "    y_true = data[part]['y'].cpu().numpy()\n",
        "    score = (\n",
        "        -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5)\n",
        "        if task_type == 'regression'\n",
        "        else sklearn.metrics.accuracy_score(y_true, y_pred.argmax(1))\n",
        "    )\n",
        "    return float(score)  # The higher -- the better.\n",
        "\n",
        "\n",
        "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lHTE9RtknNN",
        "outputId": "37ccea48-6f2d-4dcb-a4e3-503ee5ece8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score before training: 0.1499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration purposes (fast training and bad performance),\n",
        "# one can set smaller values:\n",
        "n_epochs = 50\n",
        "patience = 20\n",
        "# n_epochs = 1_000_000_000\n",
        "# patience = 16\n",
        "\n",
        "batch_size = 256\n",
        "epoch_size = math.ceil(len(train_idx) / batch_size)\n",
        "best = {\n",
        "    'val': -math.inf,\n",
        "    'test': -math.inf,\n",
        "    'epoch': -1,\n",
        "}\n",
        "remaining_patience = patience\n",
        "\n",
        "print('-' * 88 + '\\n')\n",
        "for epoch in range(n_epochs):\n",
        "    for batch_idx in tqdm(\n",
        "        torch.randperm(len(data['train']['y']), device=device).split(batch_size),\n",
        "        desc=f'Epoch {epoch}',\n",
        "        total=epoch_size,\n",
        "    ):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(apply_model('train', batch_idx), Y_train[batch_idx])\n",
        "        if grad_scaler is None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            grad_scaler.scale(loss).backward()  # type: ignore\n",
        "            grad_scaler.step(optimizer)\n",
        "            grad_scaler.update()\n",
        "\n",
        "    val_score = evaluate('val')\n",
        "    test_score = evaluate('test')\n",
        "    print(f'(val) {val_score:.4f} (test) {test_score:.4f}')\n",
        "\n",
        "    if val_score > best['val']:\n",
        "        print('ðŸŒ¸ New best epoch! ðŸŒ¸')\n",
        "        best = {'val': val_score, 'test': test_score, 'epoch': epoch}\n",
        "        remaining_patience = patience\n",
        "        torch.save(model.state_dict(), 'TabM-AG-TL-I.pth')\n",
        "    else:\n",
        "        remaining_patience -= 1\n",
        "\n",
        "    if remaining_patience < 0:\n",
        "        break\n",
        "    print()\n",
        "\n",
        "print('\\n\\nResult:')\n",
        "print(best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl3C_Ya7kqOw",
        "outputId": "7ab04e16-8bc8-4643-ac41-24b0ded3a0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 1546it [00:05, 279.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6869 (test) 0.6862\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 1546it [00:05, 280.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6945 (test) 0.6932\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 1546it [00:05, 281.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6999 (test) 0.6989\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 1546it [00:05, 283.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7051 (test) 0.7033\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 1546it [00:05, 283.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7077 (test) 0.7050\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 1546it [00:05, 283.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7077 (test) 0.7058\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 1546it [00:05, 284.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7106 (test) 0.7080\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 1546it [00:05, 283.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7116 (test) 0.7097\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 1546it [00:05, 284.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7130 (test) 0.7107\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 1546it [00:05, 284.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7141 (test) 0.7123\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 1546it [00:05, 284.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7144 (test) 0.7115\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 1546it [00:05, 284.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7160 (test) 0.7139\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 1546it [00:05, 284.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7156 (test) 0.7135\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 1546it [00:05, 283.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7152 (test) 0.7133\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 1546it [00:05, 283.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7172 (test) 0.7163\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 1546it [00:05, 282.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7175 (test) 0.7160\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 1546it [00:05, 282.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7190 (test) 0.7176\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 1546it [00:05, 282.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7174 (test) 0.7154\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 1546it [00:05, 281.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7200 (test) 0.7185\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 1546it [00:05, 281.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7181 (test) 0.7160\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 1546it [00:05, 281.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7182 (test) 0.7161\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 1546it [00:05, 281.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7197 (test) 0.7177\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 1546it [00:05, 281.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7204 (test) 0.7192\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 1546it [00:05, 280.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7213 (test) 0.7190\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 1546it [00:05, 280.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7204 (test) 0.7187\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 1546it [00:05, 280.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7213 (test) 0.7196\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 1546it [00:05, 280.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7219 (test) 0.7206\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 1546it [00:05, 279.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7225 (test) 0.7206\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 1546it [00:05, 280.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7215 (test) 0.7201\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 1546it [00:05, 280.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7223 (test) 0.7211\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 1546it [00:05, 280.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7224 (test) 0.7205\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31: 1546it [00:05, 279.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7226 (test) 0.7209\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32: 1546it [00:05, 279.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7226 (test) 0.7213\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33: 1546it [00:05, 280.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7225 (test) 0.7214\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34: 1546it [00:05, 280.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7234 (test) 0.7231\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35: 1546it [00:05, 280.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7226 (test) 0.7214\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36: 1546it [00:05, 280.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7227 (test) 0.7219\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37: 1546it [00:05, 280.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7225 (test) 0.7213\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38: 1546it [00:05, 280.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7236 (test) 0.7224\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39: 1546it [00:05, 279.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7231 (test) 0.7221\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40: 1546it [00:05, 280.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7233 (test) 0.7225\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41: 1546it [00:05, 279.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7233 (test) 0.7219\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42: 1546it [00:05, 279.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7247 (test) 0.7244\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43: 1546it [00:05, 279.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7240 (test) 0.7230\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44: 1546it [00:05, 279.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7241 (test) 0.7228\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45: 1546it [00:05, 279.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7237 (test) 0.7234\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46: 1546it [00:05, 279.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7241 (test) 0.7231\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47: 1546it [00:05, 279.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7231 (test) 0.7219\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48: 1546it [00:05, 279.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7250 (test) 0.7244\n",
            "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49: 1546it [00:05, 279.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.7239 (test) 0.7219\n",
            "\n",
            "\n",
            "\n",
            "Result:\n",
            "{'val': 0.7249685257936809, 'test': 0.7243921306900056, 'epoch': 48}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save model"
      ],
      "metadata": {
        "id": "v5FtM5U6RRB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc"
      ],
      "metadata": {
        "id": "1PCPmDbZDe2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cH9RrodtRRJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, matthews_corrcoef\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load('/content/TabM-AG-TL-I.pth'))\n",
        "model.eval()\n",
        "\n",
        "eval_batch_size = 16\n",
        "\n",
        "y_pred_all = []\n",
        "y_true_all = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx in torch.arange(len(data['test']['y']), device=device).split(eval_batch_size):\n",
        "        logits = apply_model('test', idx)\n",
        "        if task_type != 'regression':\n",
        "            logits = scipy.special.softmax(logits.cpu().numpy(), axis=-1)\n",
        "        else:\n",
        "            logits = logits.cpu().numpy()\n",
        "        y_pred_all.append(logits)\n",
        "        y_true_all.append(data['test']['y'][idx].cpu().numpy())\n",
        "\n",
        "y_pred_logits = np.vstack(y_pred_all)\n",
        "y_true = np.concatenate(y_true_all)\n",
        "\n",
        "# Mean over k\n",
        "y_pred = y_pred_logits.mean(axis=1)\n",
        "\n",
        "# Compute metrics\n",
        "if task_type == 'regression':\n",
        "    assert regression_label_stats is not None\n",
        "    y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
        "    mse = sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
        "    print(f\"RMSE: {mse ** 0.5:.4f}\")\n",
        "else:\n",
        "    y_pred_class = y_pred.argmax(axis=1)\n",
        "    results = {\n",
        "        \"TabM\": {\n",
        "            \"Test Accuracy\": accuracy_score(y_true, y_pred_class),\n",
        "            \"Test Precision\": precision_score(y_true, y_pred_class, average='binary'),\n",
        "            \"Test Recall\": recall_score(y_true, y_pred_class, average='binary'),\n",
        "            \"Test F1\": f1_score(y_true, y_pred_class, average='binary'),\n",
        "            \"Test AUC\": roc_auc_score(y_true, y_pred[:, 1]),  # assumes positive class is at index 1\n",
        "            \"Test MCC\": matthews_corrcoef(y_true, y_pred_class)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvPs5vR0nCR8",
        "outputId": "ac45a368-c764-4726-f2d0-1229f516db89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"TabM\": {\n",
            "        \"Test Accuracy\": 0.7243921306900056,\n",
            "        \"Test Precision\": 0.7362920950017724,\n",
            "        \"Test Recall\": 0.745338672867416,\n",
            "        \"Test F1\": 0.7407877655249154,\n",
            "        \"Test AUC\": 0.802025613244681,\n",
            "        \"Test MCC\": 0.44663147310618767\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDTiMrha8xqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}