{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L3W3oDJh93z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rtdl_num_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "mmuSr8ziicI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from typing import Any, Literal\n",
        "\n",
        "import rtdl_num_embeddings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "fwSqzPuLiCYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_rsqrt_uniform_(x: Tensor, d: int) -> Tensor:\n",
        "    assert d > 0\n",
        "    d_rsqrt = d**-0.5\n",
        "    return nn.init.uniform_(x, -d_rsqrt, d_rsqrt)\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def init_random_signs_(x: Tensor) -> Tensor:\n",
        "    return x.bernoulli_(0.5).mul_(2).add_(-1)\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# Modules\n",
        "# ======================================================================================\n",
        "class NLinear(nn.Module):\n",
        "    \"\"\"N linear layers applied in parallel to N disjoint parts of the input.\n",
        "\n",
        "    **Shape**\n",
        "\n",
        "    - Input: ``(B, N, in_features)``\n",
        "    - Output: ``(B, N, out_features)``\n",
        "\n",
        "    The i-th linear layer is applied to the i-th matrix of the shape (B, in_features).\n",
        "\n",
        "    Technically, this is a simplified version of delu.nn.NLinear:\n",
        "    https://yura52.github.io/delu/stable/api/generated/delu.nn.NLinear.html.\n",
        "    The difference is that this layer supports only 3D inputs\n",
        "    with exactly one batch dimension. By contrast, delu.nn.NLinear supports\n",
        "    any number of batch dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, n: int, in_features: int, out_features: int, bias: bool = True\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(n, in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.empty(n, out_features)) if bias else None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        d = self.weight.shape[-2]\n",
        "        init_rsqrt_uniform_(self.weight, d)\n",
        "        if self.bias is not None:\n",
        "            init_rsqrt_uniform_(self.bias, d)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.ndim == 3\n",
        "        assert x.shape[-(self.weight.ndim - 1) :] == self.weight.shape[:-1]\n",
        "\n",
        "        x = x.transpose(0, 1)\n",
        "        x = x @ self.weight\n",
        "        x = x.transpose(0, 1)\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias\n",
        "        return x\n",
        "\n",
        "\n",
        "class OneHotEncoding0d(nn.Module):\n",
        "    # Input:  (*, n_cat_features=len(cardinalities))\n",
        "    # Output: (*, sum(cardinalities))\n",
        "\n",
        "    def __init__(self, cardinalities: list[int]) -> None:\n",
        "        super().__init__()\n",
        "        self._cardinalities = cardinalities\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        assert x.ndim >= 1\n",
        "        assert x.shape[-1] == len(self._cardinalities)\n",
        "\n",
        "        return torch.cat(\n",
        "            [\n",
        "                # NOTE\n",
        "                # This is a quick hack to support out-of-vocabulary categories.\n",
        "                #\n",
        "                # Recall that lib.data.transform_cat encodes categorical features\n",
        "                # as follows:\n",
        "                # - In-vocabulary values receive indices from `range(cardinality)`.\n",
        "                # - All out-of-vocabulary values (i.e. new categories in validation\n",
        "                #   and test data that are not presented in the training data)\n",
        "                #   receive the index `cardinality`.\n",
        "                #\n",
        "                # As such, the line below will produce the standard one-hot encoding for\n",
        "                # known categories, and the all-zeros encoding for unknown categories.\n",
        "                # This may not be the best approach to deal with unknown values,\n",
        "                # but should be enough for our purposes.\n",
        "                nn.functional.one_hot(x[..., i], cardinality + 1)[..., :-1]\n",
        "                for i, cardinality in enumerate(self._cardinalities)\n",
        "            ],\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "\n",
        "class ScaleEnsemble(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        k: int,\n",
        "        d: int,\n",
        "        *,\n",
        "        init: Literal['ones', 'normal', 'random-signs'],\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(k, d))\n",
        "        self._weight_init = init\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        if self._weight_init == 'ones':\n",
        "            nn.init.ones_(self.weight)\n",
        "        elif self._weight_init == 'normal':\n",
        "            nn.init.normal_(self.weight)\n",
        "        elif self._weight_init == 'random-signs':\n",
        "            init_random_signs_(self.weight)\n",
        "        else:\n",
        "            raise ValueError(f'Unknown weight_init: {self._weight_init}')\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        assert x.ndim >= 2\n",
        "        return x * self.weight\n",
        "\n",
        "\n",
        "class LinearEfficientEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    This layer is a more configurable version of the \"BatchEnsemble\" layer\n",
        "    from the paper\n",
        "    \"BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning\"\n",
        "    (link: https://arxiv.org/abs/2002.06715).\n",
        "\n",
        "    First, this layer allows to select only some of the \"ensembled\" parts:\n",
        "    - the input scaling  (r_i in the BatchEnsemble paper)\n",
        "    - the output scaling (s_i in the BatchEnsemble paper)\n",
        "    - the output bias    (not mentioned in the BatchEnsemble paper,\n",
        "                          but is presented in public implementations)\n",
        "\n",
        "    Second, the initialization of the scaling weights is configurable\n",
        "    through the `scaling_init` argument.\n",
        "\n",
        "    NOTE\n",
        "    The term \"adapter\" is used in the TabM paper only to tell the story.\n",
        "    The original BatchEnsemble paper does NOT use this term. So this class also\n",
        "    avoids the term \"adapter\".\n",
        "    \"\"\"\n",
        "\n",
        "    r: None | Tensor\n",
        "    s: None | Tensor\n",
        "    bias: None | Tensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        bias: bool = True,\n",
        "        *,\n",
        "        k: int,\n",
        "        ensemble_scaling_in: bool,\n",
        "        ensemble_scaling_out: bool,\n",
        "        ensemble_bias: bool,\n",
        "        scaling_init: Literal['ones', 'random-signs'],\n",
        "    ):\n",
        "        assert k > 0\n",
        "        if ensemble_bias:\n",
        "            assert bias\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.register_parameter(\n",
        "            'r',\n",
        "            (\n",
        "                nn.Parameter(torch.empty(k, in_features))\n",
        "                if ensemble_scaling_in\n",
        "                else None\n",
        "            ),  # type: ignore[code]\n",
        "        )\n",
        "        self.register_parameter(\n",
        "            's',\n",
        "            (\n",
        "                nn.Parameter(torch.empty(k, out_features))\n",
        "                if ensemble_scaling_out\n",
        "                else None\n",
        "            ),  # type: ignore[code]\n",
        "        )\n",
        "        self.register_parameter(\n",
        "            'bias',\n",
        "            (\n",
        "                nn.Parameter(torch.empty(out_features))  # type: ignore[code]\n",
        "                if bias and not ensemble_bias\n",
        "                else nn.Parameter(torch.empty(k, out_features))\n",
        "                if ensemble_bias\n",
        "                else None\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.k = k\n",
        "        self.scaling_init = scaling_init\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init_rsqrt_uniform_(self.weight, self.in_features)\n",
        "        scaling_init_fn = {'ones': nn.init.ones_, 'random-signs': init_random_signs_}[\n",
        "            self.scaling_init\n",
        "        ]\n",
        "        if self.r is not None:\n",
        "            scaling_init_fn(self.r)\n",
        "        if self.s is not None:\n",
        "            scaling_init_fn(self.s)\n",
        "        if self.bias is not None:\n",
        "            bias_init = torch.empty(\n",
        "                # NOTE: the shape of bias_init is (out_features,) not (k, out_features).\n",
        "                # It means that all biases have the same initialization.\n",
        "                # This is similar to having one shared bias plus\n",
        "                # k zero-initialized non-shared biases.\n",
        "                self.out_features,\n",
        "                dtype=self.weight.dtype,\n",
        "                device=self.weight.device,\n",
        "            )\n",
        "            bias_init = init_rsqrt_uniform_(bias_init, self.in_features)\n",
        "            with torch.inference_mode():\n",
        "                self.bias.copy_(bias_init)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x.shape == (B, K, D)\n",
        "        assert x.ndim == 3\n",
        "\n",
        "        # >>> The equation (5) from the BatchEnsemble paper (arXiv v2).\n",
        "        if self.r is not None:\n",
        "            x = x * self.r\n",
        "        x = x @ self.weight.T\n",
        "        if self.s is not None:\n",
        "            x = x * self.s\n",
        "        # <<<\n",
        "\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VOhL9PPHiNjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        d_in: None | int = None,\n",
        "        d_out: None | int = None,\n",
        "        n_blocks: int,\n",
        "        d_block: int,\n",
        "        dropout: float,\n",
        "        activation: str = 'ReLU',\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        d_first = d_block if d_in is None else d_in\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(d_first if i == 0 else d_block, d_block),\n",
        "                    getattr(nn, activation)(),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "                for i in range(n_blocks)\n",
        "            ]\n",
        "        )\n",
        "        self.output = None if d_out is None else nn.Linear(d_block, d_out)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        if self.output is not None:\n",
        "            x = self.output(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def make_efficient_ensemble(module: nn.Module, **kwargs) -> None:\n",
        "    \"\"\"Replace torch.nn.Linear modules with LinearEfficientEnsemble.\n",
        "\n",
        "    NOTE\n",
        "    In the paper, there are no experiments with networks with normalization layers.\n",
        "    Perhaps, their trainable weights (the affine transformations) also need\n",
        "    \"ensemblification\" as in the paper about \"FiLM-Ensemble\".\n",
        "    Additional experiments are required to make conclusions.\n",
        "    \"\"\"\n",
        "    for name, submodule in list(module.named_children()):\n",
        "        if isinstance(submodule, nn.Linear):\n",
        "            module.add_module(\n",
        "                name,\n",
        "                LinearEfficientEnsemble(\n",
        "                    in_features=submodule.in_features,\n",
        "                    out_features=submodule.out_features,\n",
        "                    bias=submodule.bias is not None,\n",
        "                    **kwargs,\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            make_efficient_ensemble(submodule, **kwargs)\n",
        "\n",
        "\n",
        "def _get_first_ensemble_layer(backbone: MLP) -> LinearEfficientEnsemble:\n",
        "    if isinstance(backbone, MLP):\n",
        "        return backbone.blocks[0][0]  # type: ignore[code]\n",
        "    else:\n",
        "        raise RuntimeError(f'Unsupported backbone: {backbone}')\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def _init_first_adapter(\n",
        "    weight: Tensor,\n",
        "    distribution: Literal['normal', 'random-signs'],\n",
        "    init_sections: list[int],\n",
        ") -> None:\n",
        "    \"\"\"Initialize the first adapter.\n",
        "\n",
        "    NOTE\n",
        "    The `init_sections` argument is a historical artifact that accidentally leaked\n",
        "    from irrelevant experiments to the final models. Perhaps, the code related\n",
        "    to `init_sections` can be simply removed, but this was not tested.\n",
        "    \"\"\"\n",
        "    assert weight.ndim == 2\n",
        "    assert weight.shape[1] == sum(init_sections)\n",
        "\n",
        "    if distribution == 'normal':\n",
        "        init_fn_ = nn.init.normal_\n",
        "    elif distribution == 'random-signs':\n",
        "        init_fn_ = init_random_signs_\n",
        "    else:\n",
        "        raise ValueError(f'Unknown distribution: {distribution}')\n",
        "\n",
        "    section_bounds = [0, *torch.tensor(init_sections).cumsum(0).tolist()]\n",
        "    for i in range(len(init_sections)):\n",
        "        # NOTE\n",
        "        # As noted above, this section-based initialization is an arbitrary historical\n",
        "        # artifact. Consider the first adapter of one ensemble member.\n",
        "        # This adapter vector is implicitly split into \"sections\",\n",
        "        # where one section corresponds to one feature. The code below ensures that\n",
        "        # the adapter weights in one section are initialized with the same random value\n",
        "        # from the given distribution.\n",
        "        w = torch.empty((len(weight), 1), dtype=weight.dtype, device=weight.device)\n",
        "        init_fn_(w)\n",
        "        weight[:, section_bounds[i] : section_bounds[i + 1]] = w\n",
        "\n",
        "\n",
        "_CUSTOM_MODULES = {\n",
        "    # https://docs.python.org/3/library/stdtypes.html#definition.__name__\n",
        "    CustomModule.__name__: CustomModule\n",
        "    for CustomModule in [\n",
        "        rtdl_num_embeddings.LinearEmbeddings,\n",
        "        rtdl_num_embeddings.LinearReLUEmbeddings,\n",
        "        rtdl_num_embeddings.PeriodicEmbeddings,\n",
        "        rtdl_num_embeddings.PiecewiseLinearEmbeddings,\n",
        "        MLP,\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def make_module(type: str, *args, **kwargs) -> nn.Module:\n",
        "    Module = getattr(nn, type, None)\n",
        "    if Module is None:\n",
        "        Module = _CUSTOM_MODULES[type]\n",
        "    return Module(*args, **kwargs)\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# Optimization\n",
        "# ======================================================================================\n",
        "def default_zero_weight_decay_condition(\n",
        "    module_name: str, module: nn.Module, parameter_name: str, parameter: nn.Parameter\n",
        "):\n",
        "    from rtdl_num_embeddings import _Periodic\n",
        "\n",
        "    del module_name, parameter\n",
        "    return parameter_name.endswith('bias') or isinstance(\n",
        "        module,\n",
        "        nn.BatchNorm1d\n",
        "        | nn.LayerNorm\n",
        "        | nn.InstanceNorm1d\n",
        "        | rtdl_num_embeddings.LinearEmbeddings\n",
        "        | rtdl_num_embeddings.LinearReLUEmbeddings\n",
        "        | _Periodic,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_parameter_groups(\n",
        "    module: nn.Module,\n",
        "    zero_weight_decay_condition=default_zero_weight_decay_condition,\n",
        "    custom_groups: None | list[dict[str, Any]] = None,\n",
        ") -> list[dict[str, Any]]:\n",
        "    if custom_groups is None:\n",
        "        custom_groups = []\n",
        "    custom_params = frozenset(\n",
        "        itertools.chain.from_iterable(group['params'] for group in custom_groups)\n",
        "    )\n",
        "    assert len(custom_params) == sum(\n",
        "        len(group['params']) for group in custom_groups\n",
        "    ), 'Parameters in custom_groups must not intersect'\n",
        "    zero_wd_params = frozenset(\n",
        "        p\n",
        "        for mn, m in module.named_modules()\n",
        "        for pn, p in m.named_parameters()\n",
        "        if p not in custom_params and zero_weight_decay_condition(mn, m, pn, p)\n",
        "    )\n",
        "    default_group = {\n",
        "        'params': [\n",
        "            p\n",
        "            for p in module.parameters()\n",
        "            if p not in custom_params and p not in zero_wd_params\n",
        "        ]\n",
        "    }\n",
        "    return [\n",
        "        default_group,\n",
        "        {'params': list(zero_wd_params), 'weight_decay': 0.0},\n",
        "        *custom_groups,\n",
        "    ]"
      ],
      "metadata": {
        "id": "RghPXfSNiNl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#core"
      ],
      "metadata": {
        "id": "aoqa-XAuifN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"MLP & TabM.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        n_num_features: int,\n",
        "        cat_cardinalities: list[int],\n",
        "        n_classes: None | int,\n",
        "        backbone: dict,\n",
        "        bins: None | list[Tensor],  # For piecewise-linear encoding/embeddings.\n",
        "        num_embeddings: None | dict = None,\n",
        "        arch_type: Literal[\n",
        "            # Plain feed-forward network without any kind of ensembling.\n",
        "            'plain',\n",
        "            #\n",
        "            # TabM-mini\n",
        "            'tabm-mini',\n",
        "            #\n",
        "            # TabM-mini. The first adapter is initialized from the normal distribution.\n",
        "            # This is used in Section 5.1 of the paper.\n",
        "            'tabm-mini-normal',\n",
        "            #\n",
        "            # TabM\n",
        "            'tabm',\n",
        "            #\n",
        "            # TabM. The first adapter is initialized from the normal distribution.\n",
        "            # This variation is not used in the paper, but there is a preliminary\n",
        "            # evidence that may be a better default strategy.\n",
        "            'tabm-normal',\n",
        "        ],\n",
        "        k: None | int = None,\n",
        "    ) -> None:\n",
        "        # >>> Validate arguments.\n",
        "        assert n_num_features >= 0\n",
        "        assert n_num_features or cat_cardinalities\n",
        "        if arch_type == 'plain':\n",
        "            assert k is None\n",
        "        else:\n",
        "            assert k is not None\n",
        "            assert k > 0\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # >>> Continuous (numerical) features\n",
        "        first_adapter_sections = []  # See the comment in `_init_first_adapter`.\n",
        "\n",
        "        if n_num_features == 0:\n",
        "            assert bins is None\n",
        "            self.num_module = None\n",
        "            d_num = 0\n",
        "\n",
        "        elif num_embeddings is None:\n",
        "            assert bins is None\n",
        "            self.num_module = None\n",
        "            d_num = n_num_features\n",
        "            first_adapter_sections.extend(1 for _ in range(n_num_features))\n",
        "\n",
        "        else:\n",
        "            if bins is None:\n",
        "                self.num_module = make_module(\n",
        "                    **num_embeddings, n_features=n_num_features\n",
        "                )\n",
        "            else:\n",
        "                assert num_embeddings['type'].startswith('PiecewiseLinearEmbeddings')\n",
        "                self.num_module = make_module(**num_embeddings, bins=bins)\n",
        "            d_num = n_num_features * num_embeddings['d_embedding']\n",
        "            first_adapter_sections.extend(\n",
        "                num_embeddings['d_embedding'] for _ in range(n_num_features)\n",
        "            )\n",
        "\n",
        "        # >>> Categorical features\n",
        "        self.cat_module = (\n",
        "            OneHotEncoding0d(cat_cardinalities) if cat_cardinalities else None\n",
        "        )\n",
        "        first_adapter_sections.extend(cat_cardinalities)\n",
        "        d_cat = sum(cat_cardinalities)\n",
        "\n",
        "        # >>> Backbone\n",
        "        d_flat = d_num + d_cat\n",
        "        self.minimal_ensemble_adapter = None\n",
        "        # Any backbone can be here but we provide only MLP\n",
        "        self.backbone = make_module(d_in=d_flat, **backbone)\n",
        "\n",
        "        if arch_type != 'plain':\n",
        "            assert k is not None\n",
        "            first_adapter_init = (\n",
        "                'normal'\n",
        "                if arch_type in ('tabm-mini-normal', 'tabm-normal')\n",
        "                # For other arch_types, the initialization depends\n",
        "                # on the presense of num_embeddings.\n",
        "                else 'random-signs'\n",
        "                if num_embeddings is None\n",
        "                else 'normal'\n",
        "            )\n",
        "\n",
        "            if arch_type in ('tabm-mini', 'tabm-mini-normal'):\n",
        "                # Minimal ensemble\n",
        "                self.minimal_ensemble_adapter = ScaleEnsemble(\n",
        "                    k,\n",
        "                    d_flat,\n",
        "                    init='random-signs' if num_embeddings is None else 'normal',\n",
        "                )\n",
        "                _init_first_adapter(\n",
        "                    self.minimal_ensemble_adapter.weight,  # type: ignore[code]\n",
        "                    first_adapter_init,\n",
        "                    first_adapter_sections,\n",
        "                )\n",
        "\n",
        "            elif arch_type in ('tabm', 'tabm-normal'):\n",
        "                # Like BatchEnsemble, but all multiplicative adapters,\n",
        "                # except for the very first one, are initialized with ones.\n",
        "                make_efficient_ensemble(\n",
        "                    self.backbone,\n",
        "                    k=k,\n",
        "                    ensemble_scaling_in=True,\n",
        "                    ensemble_scaling_out=True,\n",
        "                    ensemble_bias=True,\n",
        "                    scaling_init='ones',\n",
        "                )\n",
        "                _init_first_adapter(\n",
        "                    _get_first_ensemble_layer(self.backbone).r,  # type: ignore[code]\n",
        "                    first_adapter_init,\n",
        "                    first_adapter_sections,\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f'Unknown arch_type: {arch_type}')\n",
        "\n",
        "        # >>> Output\n",
        "        d_block = backbone['d_block']\n",
        "        d_out = 1 if n_classes is None else n_classes\n",
        "        self.output = (\n",
        "            nn.Linear(d_block, d_out)\n",
        "            if arch_type == 'plain'\n",
        "            else NLinear(k, d_block, d_out)  # type: ignore[code]\n",
        "        )\n",
        "\n",
        "        # >>>\n",
        "        self.arch_type = arch_type\n",
        "        self.k = k\n",
        "\n",
        "    def forward(\n",
        "        self, x_num: None | Tensor = None, x_cat: None | Tensor = None\n",
        "    ) -> Tensor:\n",
        "        x = []\n",
        "        if x_num is not None:\n",
        "            x.append(x_num if self.num_module is None else self.num_module(x_num))\n",
        "        if x_cat is None:\n",
        "            assert self.cat_module is None\n",
        "        else:\n",
        "            assert self.cat_module is not None\n",
        "            x.append(self.cat_module(x_cat).float())\n",
        "        x = torch.column_stack([x_.flatten(1, -1) for x_ in x])\n",
        "\n",
        "        if self.k is not None:\n",
        "            x = x[:, None].expand(-1, self.k, -1)  # (B, D) -> (B, K, D)\n",
        "            if self.minimal_ensemble_adapter is not None:\n",
        "                x = self.minimal_ensemble_adapter(x)\n",
        "        else:\n",
        "            assert self.minimal_ensemble_adapter is None\n",
        "\n",
        "        x = self.backbone(x)\n",
        "        x = self.output(x)\n",
        "        if self.k is None:\n",
        "            # Adjust the output shape for plain networks to make them compatible\n",
        "            # with the rest of the script (loss, metrics, predictions, ...).\n",
        "            # (B, D_OUT) -> (B, 1, D_OUT)\n",
        "            x = x[:, None]\n",
        "        return x"
      ],
      "metadata": {
        "id": "i_EENtn5iY-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "uAHb_8F-kSwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from typing import Literal, NamedTuple\n",
        "\n",
        "import numpy as np\n",
        "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
        "import scipy.special\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "from torch import Tensor\n",
        "from tqdm.std import tqdm\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "PNeKR65kkSVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.resetwarnings()\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed + 1)\n",
        "torch.manual_seed(seed + 2)\n",
        "pass"
      ],
      "metadata": {
        "id": "Dhrc3sxXkVye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# >>> Dataset.\n",
        "TaskType = Literal['regression', 'binclass', 'multiclass']\n",
        "\n",
        "# Regression.\n",
        "task_type: TaskType = 'regression'\n",
        "n_classes = None\n",
        "dataset = sklearn.datasets.fetch_california_housing()\n",
        "X_cont: np.ndarray = dataset['data']\n",
        "Y: np.ndarray = dataset['target']\n",
        "\n",
        "# Classification.\n",
        "n_classes = 3\n",
        "assert n_classes >= 2\n",
        "task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
        "X_cont, Y = sklearn.datasets.make_classification(\n",
        "    n_samples=20000,\n",
        "    n_features=8,\n",
        "    n_classes=n_classes,\n",
        "    n_informative=4,\n",
        "    n_redundant=2,\n",
        ")\n",
        "\n",
        "task_is_regression = task_type == 'multiclass'\n",
        "\n",
        "# >>> Continuous features.\n",
        "# X_cont: np.ndarray = X_cont.astype(np.float32)\n",
        "# n_cont_features = X_cont.shape[1]\n",
        "\n",
        "# >>> Categorical features.\n",
        "# NOTE: the above datasets do not have categorical features, however,\n",
        "# for the demonstration purposes, it is possible to generate them.\n",
        "cat_cardinalities = [\n",
        "    # NOTE: uncomment the two lines below to add two categorical features.\n",
        "    # 4,  # Allowed values: [0, 1, 2, 3].\n",
        "    # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
        "]\n",
        "X_cat = (\n",
        "    np.column_stack(\n",
        "        [np.random.randint(0, c, (len(X_cont),)) for c in cat_cardinalities]\n",
        "    )\n",
        "    if cat_cardinalities\n",
        "    else None\n",
        ")\n",
        "\n",
        "# >>> Labels.\n",
        "if task_type == 'regression':\n",
        "    Y = Y.astype(np.float32)\n",
        "else:\n",
        "    assert n_classes is not None\n",
        "    Y = Y.astype(np.int64)\n",
        "    assert set(Y.tolist()) == set(\n",
        "        range(n_classes)\n",
        "    ), 'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
        "\n",
        "# >>> Split the dataset.\n",
        "all_idx = np.arange(len(Y))\n",
        "trainval_idx, test_idx = sklearn.model_selection.train_test_split(\n",
        "    all_idx, train_size=0.8\n",
        ")\n",
        "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
        "    trainval_idx, train_size=0.8\n",
        ")\n",
        "data_numpy = {\n",
        "    'train': {'x_cont': X_cont[train_idx], 'y': Y[train_idx]},\n",
        "    'val': {'x_cont': X_cont[val_idx], 'y': Y[val_idx]},\n",
        "    'test': {'x_cont': X_cont[test_idx], 'y': Y[test_idx]},\n",
        "}\n",
        "if X_cat is not None:\n",
        "    data_numpy['train']['x_cat'] = X_cat[train_idx]\n",
        "    data_numpy['val']['x_cat'] = X_cat[val_idx]\n",
        "    data_numpy['test']['x_cat'] = X_cat[test_idx]"
      ],
      "metadata": {
        "id": "TsTTWw6vkYdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e89654-a181-4e25-cbf4-dec2259c6932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature preprocessing.\n",
        "# NOTE\n",
        "# The choice between preprocessing strategies depends on a task and a model.\n",
        "\n",
        "# Simple preprocessing strategy.\n",
        "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
        "#     data_numpy['train']['x_cont']\n",
        "# )\n",
        "\n",
        "# Advanced preprocessing strategy.\n",
        "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
        "X_cont_train_numpy = data_numpy['train']['x_cont']\n",
        "noise = (\n",
        "    np.random.default_rng(0)\n",
        "    .normal(0.0, 1e-5, X_cont_train_numpy.shape)\n",
        "    .astype(X_cont_train_numpy.dtype)\n",
        ")\n",
        "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
        "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
        "    output_distribution='normal',\n",
        "    subsample=10**9,\n",
        ").fit(X_cont_train_numpy + noise)\n",
        "del X_cont_train_numpy\n",
        "\n",
        "# Apply the preprocessing.\n",
        "for part in data_numpy:\n",
        "    data_numpy[part]['x_cont'] = preprocessing.transform(data_numpy[part]['x_cont'])\n",
        "\n",
        "\n",
        "# Label preprocessing.\n",
        "class RegressionLabelStats(NamedTuple):\n",
        "    mean: float\n",
        "    std: float\n",
        "\n",
        "\n",
        "Y_train = data_numpy['train']['y'].copy()\n",
        "if task_type == 'regression':\n",
        "    # For regression tasks, it is highly recommended to standardize the training labels.\n",
        "    regression_label_stats = RegressionLabelStats(\n",
        "        Y_train.mean().item(), Y_train.std().item()\n",
        "    )\n",
        "    Y_train = (Y_train - regression_label_stats.mean) / regression_label_stats.std\n",
        "else:\n",
        "    regression_label_stats = None"
      ],
      "metadata": {
        "id": "z-HN9UIekepN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLdnaNw-80y",
        "outputId": "742b22c1-4761-4cf1-d412-b0ca48f8d6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CferhQ2v30rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *pth*"
      ],
      "metadata": {
        "id": "WPM-I_P6SH1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv(\"/content/drive/MyDrive/TL-II/AGORA/50/X_train_TL-II.csv\")\n",
        "X_val = pd.read_csv(\"/content/drive/MyDrive/TL-II/AGORA/50/X_val_TL-II.csv\")\n",
        "X_test = pd.read_csv(\"/content/drive/MyDrive/TL-II/AGORA/50/X_test_TL-II.csv\")\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/TL-II/AGORA/50/y_train_TL-II.csv\")\n",
        "y_val = pd.read_csv(\"/content/drive/MyDrive/TL-II/AGORA/50/y_val_TL-II.csv\")\n",
        "y_test = pd.read_csv(\"/content/drive/MyDrive/TL-II/AGORA/50/y_test_TL-II.csv\")"
      ],
      "metadata": {
        "id": "QAPMOg3n_U5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_numpy = {\n",
        "    'train': {'x_cont': X_train.values, 'y': y_train.values},\n",
        "    'val': {'x_cont': X_val.values, 'y': y_val.values},\n",
        "    'test': {'x_cont': X_test.values, 'y': y_test.values},\n",
        "}"
      ],
      "metadata": {
        "id": "WFu0QaAC_qdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.replace(2, 0)\n",
        "y_val = y_val.replace(2, 0)\n",
        "y_test = y_test.replace(2, 0)\n",
        "\n",
        "y_train = y_train.replace(3, 1)\n",
        "y_val = y_val.replace(3, 1)\n",
        "y_test = y_test.replace(3, 1)\n",
        "\n",
        "y_train = y_train.replace(4, 2)\n",
        "y_val = y_val.replace(4, 2)\n",
        "y_test = y_test.replace(4, 2)"
      ],
      "metadata": {
        "id": "fNtcToMs3SZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert data to tensors\n",
        "data = {\n",
        "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
        "    for part in data_numpy\n",
        "}\n",
        "Y_train = torch.as_tensor(y_train.values, device=device)\n",
        "if task_type == 'regression':\n",
        "    for part in data:\n",
        "        data[part]['y'] = data[part]['y'].float()\n",
        "    Y_train = Y_train.float()\n",
        "\n",
        "# Automatic mixed precision (AMP)\n",
        "# torch.float16 is implemented for completeness,\n",
        "# but it was not tested in the project,\n",
        "# so torch.bfloat16 is used by default.\n",
        "amp_dtype = (\n",
        "    torch.bfloat16\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else torch.float16\n",
        "    if torch.cuda.is_available()\n",
        "    else None\n",
        ")\n",
        "# Changing False to True will result in faster training on compatible hardware.\n",
        "amp_enabled = False and amp_dtype is not None\n",
        "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
        "\n",
        "# torch.compile\n",
        "compile_model = False\n",
        "\n",
        "# fmt: off\n",
        "print(\n",
        "    f'Device:        {device.type.upper()}'\n",
        "    f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
        "    f'\\ntorch.compile: {compile_model}'\n",
        ")\n",
        "# fmt: on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaf6AiJ__o4Y",
        "outputId": "55b83da9-f7ff-4c2f-a8b2-d5f414215d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:        CUDA\n",
            "AMP:           False (dtype: torch.bfloat16)\n",
            "torch.compile: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose one of the two configurations below.\n",
        "\n",
        "# TabM\n",
        "arch_type = 'tabm'\n",
        "bins = None\n",
        "\n",
        "# TabM-mini with the piecewise-linear embeddings.\n",
        "# arch_type = 'tabm-mini'\n",
        "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
        "\n",
        "model = Model(\n",
        "    n_num_features=X_train.shape[1],\n",
        "    cat_cardinalities=[],\n",
        "    n_classes=n_classes,\n",
        "    backbone={\n",
        "        'type': 'MLP',\n",
        "        'n_blocks': 3 if bins is None else 2,\n",
        "        'd_block': 512,\n",
        "        'dropout': 0.1,\n",
        "    },\n",
        "    bins=bins,\n",
        "    num_embeddings=(\n",
        "        None\n",
        "        if bins is None\n",
        "        else {\n",
        "            'type': 'PiecewiseLinearEmbeddings',\n",
        "            'd_embedding': 16,\n",
        "            'activation': False,\n",
        "            'version': 'B',\n",
        "        }\n",
        "    ),\n",
        "    arch_type=arch_type,\n",
        "    k=32,\n",
        ").to(device)\n",
        "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
        "\n",
        "if compile_model:\n",
        "    # NOTE\n",
        "    # `torch.compile` is intentionally called without the `mode` argument\n",
        "    # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
        "    model = torch.compile(model)\n",
        "    evaluation_mode = torch.no_grad\n",
        "else:\n",
        "    evaluation_mode = torch.inference_mode"
      ],
      "metadata": {
        "id": "ExdUqKQ0kjG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfa85db-b7d2-4568-e4de-c1a6243dd50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
        "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
        "    return (\n",
        "        model(\n",
        "            data[part]['x_cont'][idx],\n",
        "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
        "        )\n",
        "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
        "        .float()\n",
        "    )\n",
        "\n",
        "\n",
        "base_loss_fn = F.mse_loss if task_type == 'regression' else F.cross_entropy\n",
        "\n",
        "\n",
        "def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
        "    # TabM produces k predictions per object. Each of them must be trained separately.\n",
        "    # (regression)     y_pred.shape == (batch_size, k)\n",
        "    # (classification) y_pred.shape == (batch_size, k, n_classes)\n",
        "    k = y_pred.shape[-1 if task_type == 'regression' else -2]\n",
        "    return base_loss_fn(y_pred.flatten(0, 1), y_true.repeat_interleave(k))\n",
        "\n",
        "\n",
        "@evaluation_mode()\n",
        "def evaluate(part: str) -> float:\n",
        "    model.eval()\n",
        "\n",
        "    # When using torch.compile, you may need to reduce the evaluation batch size.\n",
        "    eval_batch_size = 8096\n",
        "    y_pred: np.ndarray = (\n",
        "        torch.cat(\n",
        "            [\n",
        "                apply_model(part, idx)\n",
        "                for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
        "                    eval_batch_size\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "    )\n",
        "    if task_type == 'regression':\n",
        "        # Transform the predictions back to the original label space.\n",
        "        assert regression_label_stats is not None\n",
        "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
        "\n",
        "    # Compute the mean of the k predictions.\n",
        "    if task_type != 'regression':\n",
        "        # For classification, the mean must be computed in the probabily space.\n",
        "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
        "    y_pred = y_pred.mean(1)\n",
        "\n",
        "    y_true = data[part]['y'].cpu().numpy()\n",
        "    score = (\n",
        "        -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5)\n",
        "        if task_type == 'regression'\n",
        "        else sklearn.metrics.accuracy_score(y_true, y_pred.argmax(1))\n",
        "    )\n",
        "    return float(score)  # The higher -- the better.\n",
        "\n",
        "\n",
        "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lHTE9RtknNN",
        "outputId": "9a8bd4be-6478-42c4-eff1-c60c0df0e7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score before training: 0.0032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration purposes (fast training and bad performance),\n",
        "# one can set smaller values:\n",
        "n_epochs = 50\n",
        "patience = 16\n",
        "# n_epochs = 1_000_000_000\n",
        "# patience = 16\n",
        "\n",
        "batch_size = 256\n",
        "epoch_size = math.ceil(len(train_idx) / batch_size)\n",
        "best = {\n",
        "    'val': -math.inf,\n",
        "    'test': -math.inf,\n",
        "    'epoch': -1,\n",
        "}\n",
        "# Early stopping: the training stops when\n",
        "# there are more than `patience` consequtive bad updates.\n",
        "patience = 16\n",
        "remaining_patience = patience\n",
        "\n",
        "print('-' * 88 + '\\n')\n",
        "for epoch in range(n_epochs):\n",
        "    for batch_idx in tqdm(\n",
        "        torch.randperm(len(data['train']['y']), device=device).split(batch_size),\n",
        "        desc=f'Epoch {epoch}',\n",
        "        total=epoch_size,\n",
        "    ):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(apply_model('train', batch_idx), Y_train[batch_idx])\n",
        "        if grad_scaler is None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            grad_scaler.scale(loss).backward()  # type: ignore\n",
        "            grad_scaler.step(optimizer)\n",
        "            grad_scaler.update()\n",
        "\n",
        "    val_score = evaluate('val')\n",
        "    test_score = evaluate('test')\n",
        "    print(f'(val) {val_score:.4f} (test) {test_score:.4f}')\n",
        "\n",
        "    if val_score > best['val']:\n",
        "        print(' New best epoch! ')\n",
        "        best = {'val': val_score, 'test': test_score, 'epoch': epoch}\n",
        "        remaining_patience = patience\n",
        "    else:\n",
        "        remaining_patience -= 1\n",
        "\n",
        "    if remaining_patience < 0:\n",
        "        break\n",
        "\n",
        "    print()\n",
        "\n",
        "print('\\n\\nResult:')\n",
        "print(best)"
      ],
      "metadata": {
        "id": "gl3C_Ya7kqOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save model"
      ],
      "metadata": {
        "id": "v5FtM5U6RRB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save torch model\n",
        "\n",
        "torch.save(model.state_dict(), 'TabM-AG-TL-II.pth')"
      ],
      "metadata": {
        "id": "cH9RrodtRRJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('TabM_multi.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvPs5vR0nCR8",
        "outputId": "16c4b905-3f51-4375-dee3-632a524043f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-306a9a569608>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('TabM_multi.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score"
      ],
      "metadata": {
        "id": "-rMGQnLanGY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, mean_squared_error\n",
        ")\n",
        "\n",
        "def evaluate_and_save_metrics(part: str, output_file=\"metrics_results.json\") -> float:\n",
        "    model.eval()\n",
        "\n",
        "    eval_batch_size = 128\n",
        "    y_true_list, y_pred_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in torch.arange(len(data[part]['y']), device=device).split(eval_batch_size):\n",
        "            y_batch = data[part]['y'][idx].cpu().numpy()\n",
        "            y_pred_batch = apply_model(part, idx).cpu().numpy()\n",
        "\n",
        "            y_true_list.append(y_batch)\n",
        "            y_pred_list.append(y_pred_batch)\n",
        "\n",
        "    y_true = np.concatenate(y_true_list, axis=0)\n",
        "    y_pred = np.concatenate(y_pred_list, axis=0)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    if task_type == 'regression':\n",
        "        assert regression_label_stats is not None\n",
        "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
        "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "        results[\"RMSE\"] = rmse\n",
        "        score = -rmse\n",
        "    else:\n",
        "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
        "        y_pred = y_pred.mean(axis=1)\n",
        "        y_pred_classes = y_pred.argmax(axis=1)\n",
        "\n",
        "        results[\"Test Accuracy\"] = accuracy_score(y_true, y_pred_classes)\n",
        "        results[\"Test AUC\"] = roc_auc_score(y_true, y_pred, multi_class='ovr')\n",
        "        results[\"Test Precision\"] = precision_score(y_true, y_pred_classes, average=None, zero_division=0).tolist()\n",
        "        results[\"Test Recall\"] = recall_score(y_true, y_pred_classes, average=None, zero_division=0).tolist()\n",
        "        results[\"Test F1\"] = f1_score(y_true, y_pred_classes, average=None, zero_division=0).tolist()\n",
        "        results[\"Test MCC\"] = [\n",
        "            matthews_corrcoef((y_true == i).astype(int), (y_pred_classes == i).astype(int))\n",
        "            for i in range(y_pred.shape[1])\n",
        "        ]\n",
        "        score = results[\"Test Accuracy\"]\n",
        "\n",
        "    # Save as JSON\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    # Print nicely\n",
        "    print(json.dumps(results, indent=4))\n",
        "\n",
        "    return score\n",
        "\n",
        "# Usage\n",
        "print(f\"Test score before training: {evaluate_and_save_metrics('test'):.4f}\")"
      ],
      "metadata": {
        "id": "R1i5H7x-n6P_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa5fbc4-ae6e-4824-fae6-926791807ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Test Accuracy\": 0.6783529711041476,\n",
            "    \"Test AUC\": 0.817533630689106,\n",
            "    \"Test Precision\": [\n",
            "        0.6126172105172806,\n",
            "        0.5711882229232387,\n",
            "        0.795170558553396\n",
            "    ],\n",
            "    \"Test Recall\": [\n",
            "        0.8851393022819296,\n",
            "        0.05709840856055669,\n",
            "        0.6696694885148767\n",
            "    ],\n",
            "    \"Test F1\": [\n",
            "        0.7240850774465798,\n",
            "        0.10381866136615572,\n",
            "        0.7270438329349183\n",
            "    ],\n",
            "    \"Test MCC\": [\n",
            "        0.4709005590875215,\n",
            "        0.1486698923628882,\n",
            "        0.5549129561775952\n",
            "    ]\n",
            "}\n",
            "Test score before training: 0.6784\n"
          ]
        }
      ]
    }
  ]
}